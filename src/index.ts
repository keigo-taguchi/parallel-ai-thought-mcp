import 'dotenv/config';
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { z } from "zod";

// AIæä¾›è€…ã®ç¨®é¡
type AIProvider = 'openai' | 'anthropic' | 'gemini' | 'deepseek' | 'ollama';

// AIæ€è€ƒã‚¿ã‚¹ã‚¯ã®å®šç¾©
interface AIThoughtTask {
  id: string;
  prompt: string;
  provider: AIProvider;
  model?: string;
  temperature?: number;
  maxTokens?: number;
  metadata?: Record<string, any>;
}

// AIå¿œç­”ã®å®šç¾©
interface AIThoughtResponse {
  taskId: string;
  provider: AIProvider;
  model: string;
  response: string;
  usage?: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
  };
  timestamp: number;
  duration: number;
}

// ä¸¦è¡Œæ€è€ƒçµæœ
interface ParallelThoughtResult {
  sessionId: string;
  tasks: AIThoughtTask[];
  responses: AIThoughtResponse[];
  summary?: string;
  consensus?: string;
  totalDuration: number;
  timestamp: number;
}

// AIæä¾›è€…ã®è¨­å®š
interface AIProviderConfig {
  apiKey: string;
  baseURL?: string;
  defaultModel?: string;
}

class AIInterface {
  private configs: Map<AIProvider, AIProviderConfig> = new Map();

  constructor() {
    this.initializeProviders();
  }

  private initializeProviders() {
    // ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIè¨­å®šã‚’èª­ã¿è¾¼ã¿
    if (process.env.OPENAI_API_KEY) {
      this.configs.set('openai', {
        apiKey: process.env.OPENAI_API_KEY,
        baseURL: process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1',
        defaultModel: 'gpt-4'
      });
    }

    if (process.env.ANTHROPIC_API_KEY) {
      this.configs.set('anthropic', {
        apiKey: process.env.ANTHROPIC_API_KEY,
        baseURL: process.env.ANTHROPIC_BASE_URL || 'https://api.anthropic.com',
        defaultModel: 'claude-3-5-sonnet-20241022'
      });
    }

    if (process.env.GEMINI_API_KEY) {
      this.configs.set('gemini', {
        apiKey: process.env.GEMINI_API_KEY,
        baseURL: process.env.GEMINI_BASE_URL || 'https://generativelanguage.googleapis.com',
        defaultModel: 'gemini-pro'
      });
    }

    if (process.env.DEEPSEEK_API_KEY) {
      this.configs.set('deepseek', {
        apiKey: process.env.DEEPSEEK_API_KEY,
        baseURL: process.env.DEEPSEEK_BASE_URL || 'https://api.deepseek.com',
        defaultModel: 'deepseek-chat'
      });
    }

    if (process.env.OLLAMA_BASE_URL) {
      this.configs.set('ollama', {
        apiKey: 'not-needed',
        baseURL: process.env.OLLAMA_BASE_URL || 'http://localhost:11434',
        defaultModel: 'llama3.2'
      });
    }
  }

  async executeTask(task: AIThoughtTask): Promise<AIThoughtResponse> {
    const startTime = Date.now();
    const config = this.configs.get(task.provider);
    
    if (!config) {
      throw new Error(`Provider ${task.provider} is not configured`);
    }

    try {
      const response = await this.callAI(task, config);
      const endTime = Date.now();

      return {
        taskId: task.id,
        provider: task.provider,
        model: task.model || config.defaultModel || 'unknown',
        response: response.content,
        usage: response.usage,
        timestamp: endTime,
        duration: endTime - startTime
      };
    } catch (error) {
      console.error(`Error executing task ${task.id} with ${task.provider}:`, error);
      const endTime = Date.now();
      
      return {
        taskId: task.id,
        provider: task.provider,
        model: task.model || config.defaultModel || 'unknown',
        response: `Error: ${error instanceof Error ? error.message : 'Unknown error'}`,
        timestamp: endTime,
        duration: endTime - startTime
      };
    }
  }

  private async callAI(task: AIThoughtTask, config: AIProviderConfig): Promise<{
    content: string;
    usage?: { promptTokens?: number; completionTokens?: number; totalTokens?: number };
  }> {
    const model = task.model || config.defaultModel;

    switch (task.provider) {
      case 'openai':
        return this.callOpenAI(task, config, model!);
      case 'anthropic':
        return this.callAnthropic(task, config, model!);
      case 'gemini':
        return this.callGemini(task, config, model!);
      case 'deepseek':
        return this.callDeepSeek(task, config, model!);
      case 'ollama':
        return this.callOllama(task, config, model!);
      default:
        throw new Error(`Unsupported provider: ${task.provider}`);
    }
  }

  private async callOpenAI(task: AIThoughtTask, config: AIProviderConfig, model: string) {
    const response = await fetch(`${config.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${config.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: task.prompt }],
        temperature: task.temperature || 0.7,
        max_tokens: task.maxTokens || 2000
      })
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0]?.message?.content || '',
      usage: data.usage ? {
        promptTokens: data.usage.prompt_tokens,
        completionTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens
      } : undefined
    };
  }

  private async callAnthropic(task: AIThoughtTask, config: AIProviderConfig, model: string) {
    const response = await fetch(`${config.baseURL}/v1/messages`, {
      method: 'POST',
      headers: {
        'x-api-key': config.apiKey,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: task.prompt }],
        temperature: task.temperature || 0.7,
        max_tokens: task.maxTokens || 2000
      })
    });

    if (!response.ok) {
      throw new Error(`Anthropic API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.content[0]?.text || '',
      usage: data.usage ? {
        promptTokens: data.usage.input_tokens,
        completionTokens: data.usage.output_tokens,
        totalTokens: (data.usage.input_tokens || 0) + (data.usage.output_tokens || 0)
      } : undefined
    };
  }

  private async callGemini(task: AIThoughtTask, config: AIProviderConfig, model: string) {
    const response = await fetch(`${config.baseURL}/v1/models/${model}:generateContent?key=${config.apiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        contents: [{ parts: [{ text: task.prompt }] }],
        generationConfig: {
          temperature: task.temperature || 0.7,
          maxOutputTokens: task.maxTokens || 2000
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.candidates?.[0]?.content?.parts?.[0]?.text || '',
      usage: data.usageMetadata ? {
        promptTokens: data.usageMetadata.promptTokenCount,
        completionTokens: data.usageMetadata.candidatesTokenCount,
        totalTokens: data.usageMetadata.totalTokenCount
      } : undefined
    };
  }

  private async callDeepSeek(task: AIThoughtTask, config: AIProviderConfig, model: string) {
    const response = await fetch(`${config.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${config.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: task.prompt }],
        temperature: task.temperature || 0.7,
        max_tokens: task.maxTokens || 2000
      })
    });

    if (!response.ok) {
      throw new Error(`DeepSeek API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0]?.message?.content || '',
      usage: data.usage ? {
        promptTokens: data.usage.prompt_tokens,
        completionTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens
      } : undefined
    };
  }

  private async callOllama(task: AIThoughtTask, config: AIProviderConfig, model: string) {
    const response = await fetch(`${config.baseURL}/api/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model,
        prompt: task.prompt,
        stream: false,
        options: {
          temperature: task.temperature || 0.7,
          num_predict: task.maxTokens || 2000
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.response || '',
      usage: undefined // Ollamaã¯è©³ç´°ãªä½¿ç”¨é‡ã‚’è¿”ã•ãªã„
    };
  }

  getAvailableProviders(): AIProvider[] {
    return Array.from(this.configs.keys());
  }
}

export { AIInterface, AIThoughtTask, AIThoughtResponse, ParallelThoughtResult, AIProvider };

// ä¸¦è¡Œã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
class ParallelThoughtManager {
  public aiInterface: AIInterface;
  private sessions: Map<string, ParallelThoughtResult> = new Map();

  constructor() {
    this.aiInterface = new AIInterface();
  }

  async executeParallelThoughts(
    sessionId: string,
    basePrompt: string,
    providers: AIProvider[],
    options?: {
      variants?: string[];
      temperature?: number;
      maxTokens?: number;
      customModels?: Partial<Record<AIProvider, string>>;
    }
  ): Promise<ParallelThoughtResult> {
    const startTime = Date.now();
    
    // ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
    const tasks: AIThoughtTask[] = [];
    const variants = options?.variants || [''];
    
    for (const provider of providers) {
      for (let i = 0; i < variants.length; i++) {
        const variant = variants[i];
        const prompt = variant ? `${basePrompt}\n\n${variant}` : basePrompt;
        
        tasks.push({
          id: `${provider}-${i}`,
          prompt,
          provider,
          model: options?.customModels?.[provider],
          temperature: options?.temperature,
          maxTokens: options?.maxTokens
        });
      }
    }

    // ä¸¦è¡Œå®Ÿè¡Œ
    const responses = await Promise.all(
      tasks.map(task => this.aiInterface.executeTask(task))
    );

    const endTime = Date.now();
    const result: ParallelThoughtResult = {
      sessionId,
      tasks,
      responses,
      totalDuration: endTime - startTime,
      timestamp: endTime
    };

    // çµæœã‚’ä¿å­˜
    this.sessions.set(sessionId, result);

    return result;
  }

  async summarizeResponses(sessionId: string, summaryProvider: AIProvider = 'anthropic'): Promise<string> {
    const session = this.sessions.get(sessionId);
    if (!session) {
      throw new Error(`Session ${sessionId} not found`);
    }

    const responsesText = session.responses
      .map(r => `**${r.provider} (${r.model})**:\n${r.response}`)
      .join('\n\n---\n\n');

    const summaryPrompt = `ä»¥ä¸‹ã¯åŒã˜è³ªå•ã«å¯¾ã™ã‚‹è¤‡æ•°ã®AIã®å›ç­”ã§ã™ã€‚ã“ã‚Œã‚‰ã®å›ç­”ã‚’åˆ†æã—ã€å…±é€šç‚¹ã€ç›¸é•ç‚¹ã€ãã—ã¦ç·åˆçš„ãªæ´å¯Ÿã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

${responsesText}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
1. å…±é€šã™ã‚‹è¦‹è§£
2. ç•°ãªã‚‹è¦³ç‚¹
3. ç·åˆçš„ãªçµè«–`;

    const summaryTask: AIThoughtTask = {
      id: `summary-${sessionId}`,
      prompt: summaryPrompt,
      provider: summaryProvider,
      temperature: 0.3
    };

    const summaryResponse = await this.aiInterface.executeTask(summaryTask);
    session.summary = summaryResponse.response;

    return summaryResponse.response;
  }

  async findConsensus(sessionId: string, consensusProvider: AIProvider = 'anthropic'): Promise<string> {
    const session = this.sessions.get(sessionId);
    if (!session) {
      throw new Error(`Session ${sessionId} not found`);
    }

    const responsesText = session.responses
      .map(r => `**${r.provider}**: ${r.response}`)
      .join('\n\n');

    const consensusPrompt = `ä»¥ä¸‹ã¯åŒã˜è³ªå•ã«å¯¾ã™ã‚‹è¤‡æ•°ã®AIã®å›ç­”ã§ã™ã€‚ã“ã‚Œã‚‰ã®å›ç­”ã‹ã‚‰æœ€ã‚‚åˆç†çš„ã§ä¿¡é ¼æ€§ã®é«˜ã„çµè«–ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚

${responsesText}

åˆæ„ã§ãã‚‹çµè«–ã‚’ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ï¼š`;

    const consensusTask: AIThoughtTask = {
      id: `consensus-${sessionId}`,
      prompt: consensusPrompt,
      provider: consensusProvider,
      temperature: 0.1
    };

    const consensusResponse = await this.aiInterface.executeTask(consensusTask);
    session.consensus = consensusResponse.response;

    return consensusResponse.response;
  }

  getSession(sessionId: string): ParallelThoughtResult | undefined {
    return this.sessions.get(sessionId);
  }

  getAllSessions(): ParallelThoughtResult[] {
    return Array.from(this.sessions.values());
  }

  getAvailableProviders(): AIProvider[] {
    return this.aiInterface.getAvailableProviders();
  }

  // å¤–éƒ¨ã‹ã‚‰å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰
  async executeTask(task: AIThoughtTask): Promise<AIThoughtResponse> {
    return this.aiInterface.executeTask(task);
  }
}

// MCPã‚µãƒ¼ãƒãƒ¼ã®åˆæœŸåŒ–
const thoughtManager = new ParallelThoughtManager();

const server = new McpServer({
  name: "parallel-ai-thought",
  version: "1.0.0",
});

// ãƒ„ãƒ¼ãƒ«1: ä¸¦è¡ŒAIæ€è€ƒå®Ÿè¡Œ
server.registerTool(
  "parallel-ai-think",
  {
    title: "ä¸¦è¡ŒAIæ€è€ƒ",
    description: "è¤‡æ•°ã®AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŒã˜è³ªå•ã‚’ä¸¦è¡Œã§æŠ•ã’ã¦ã€ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰ã®å›ç­”ã‚’å¾—ã‚‹",
    inputSchema: {
      prompt: z.string().describe("AIã«æŠ•ã’ã‚‹è³ªå•ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"),
      providers: z.array(z.enum(['openai', 'anthropic', 'gemini', 'deepseek', 'ollama'])).optional().describe("ä½¿ç”¨ã™ã‚‹AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰"),
      sessionId: z.string().optional().describe("ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆæœªæŒ‡å®šã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰"),
      variants: z.array(z.string()).optional().describe("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç•°ãªã‚‹è§’åº¦ã‹ã‚‰ã®è³ªå•ï¼‰"),
      temperature: z.number().min(0).max(2).optional().describe("å›ç­”ã®å‰µé€ æ€§ï¼ˆ0-2ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.7ï¼‰"),
      maxTokens: z.number().min(1).max(4000).optional().describe("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2000ï¼‰"),
      customModels: z.record(z.string()).optional().describe("ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¯ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«æŒ‡å®š")
    },
  },
  async ({ prompt, providers, sessionId, variants, temperature, maxTokens, customModels }) => {
    const availableProviders = thoughtManager.getAvailableProviders();
    
    if (availableProviders.length === 0) {
      throw new Error("åˆ©ç”¨å¯èƒ½ãªAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã§APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚");
    }

    const targetProviders = providers && providers.length > 0 
      ? providers.filter(p => availableProviders.includes(p))
      : availableProviders;

    if (targetProviders.length === 0) {
      throw new Error("æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚");
    }

    const finalSessionId = sessionId || `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;

    try {
      const result = await thoughtManager.executeParallelThoughts(
        finalSessionId,
        prompt,
        targetProviders,
        {
          variants,
          temperature,
          maxTokens,
          customModels: customModels as Partial<Record<AIProvider, string>>
        }
      );

      return {
        content: [
          {
            type: "text",
            text: JSON.stringify({
              sessionId: result.sessionId,
              prompt,
              providers: targetProviders,
              responses: result.responses.map(r => ({
                provider: r.provider,
                model: r.model,
                response: r.response,
                duration: r.duration,
                usage: r.usage
              })),
              totalDuration: result.totalDuration,
              timestamp: result.timestamp
            }, null, 2)
          }
        ]
      };
    } catch (error) {
      throw new Error(`ä¸¦è¡Œæ€è€ƒã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
);

// ãƒ„ãƒ¼ãƒ«2: æ€è€ƒçµæœã®è¦ç´„
server.registerTool(
  "summarize-thoughts",
  {
    title: "æ€è€ƒçµæœè¦ç´„",
    description: "ä¸¦è¡ŒAIæ€è€ƒã®çµæœã‚’åˆ†æã—ã¦è¦ç´„ã™ã‚‹",
    inputSchema: {
      sessionId: z.string().describe("è¦ç´„ã—ãŸã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ID"),
      summaryProvider: z.enum(['openai', 'anthropic', 'gemini', 'deepseek', 'ollama']).optional().describe("è¦ç´„ã«ä½¿ç”¨ã™ã‚‹AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: anthropicï¼‰")
    },
  },
  async ({ sessionId, summaryProvider }) => {
    try {
      const summary = await thoughtManager.summarizeResponses(sessionId, summaryProvider);
      const session = thoughtManager.getSession(sessionId);

      return {
        content: [
          {
            type: "text",
            text: JSON.stringify({
              sessionId,
              summary,
              originalResponses: session?.responses.length || 0,
              timestamp: Date.now()
            }, null, 2)
          }
        ]
      };
    } catch (error) {
      throw new Error(`è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
);

// ãƒ„ãƒ¼ãƒ«3: åˆæ„ç‚¹æŠ½å‡º
server.registerTool(
  "find-consensus",
  {
    title: "åˆæ„ç‚¹æŠ½å‡º",
    description: "è¤‡æ•°ã®AIå›ç­”ã‹ã‚‰åˆæ„ã§ãã‚‹çµè«–ã‚’å°å‡ºã™ã‚‹",
    inputSchema: {
      sessionId: z.string().describe("åˆ†æã—ãŸã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ID"),
      consensusProvider: z.enum(['openai', 'anthropic', 'gemini', 'deepseek', 'ollama']).optional().describe("åˆæ„åˆ†æã«ä½¿ç”¨ã™ã‚‹AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: anthropicï¼‰")
    },
  },
  async ({ sessionId, consensusProvider }) => {
    try {
      const consensus = await thoughtManager.findConsensus(sessionId, consensusProvider);
      const session = thoughtManager.getSession(sessionId);

      return {
        content: [
          {
            type: "text",
            text: JSON.stringify({
              sessionId,
              consensus,
              originalResponses: session?.responses.length || 0,
              timestamp: Date.now()
            }, null, 2)
          }
        ]
      };
    } catch (error) {
      throw new Error(`åˆæ„ç‚¹ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
);

// ãƒ„ãƒ¼ãƒ«4: ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±å–å¾—
server.registerTool(
  "get-session-info",
  {
    title: "ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±å–å¾—",
    description: "ç‰¹å®šã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã™ã‚‹",
    inputSchema: {
      sessionId: z.string().describe("æƒ…å ±ã‚’å–å¾—ã—ãŸã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ID")
    },
  },
  async ({ sessionId }) => {
    const session = thoughtManager.getSession(sessionId);
    
    if (!session) {
      throw new Error(`ã‚»ãƒƒã‚·ãƒ§ãƒ³ ${sessionId} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“`);
    }

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify(session, null, 2)
        }
      ]
    };
  }
);

// ãƒ„ãƒ¼ãƒ«5: åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§
server.registerTool(
  "list-providers",
  {
    title: "ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§",
    description: "ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—ã™ã‚‹",
    inputSchema: {},
  },
  async () => {
    const providers = thoughtManager.getAvailableProviders();

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify({
            availableProviders: providers,
            totalCount: providers.length,
            timestamp: Date.now()
          }, null, 2)
        }
      ]
    };
  }
);

// ãƒ„ãƒ¼ãƒ«6: å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§
server.registerTool(
  "list-sessions",
  {
    title: "ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§",
    description: "ã“ã‚Œã¾ã§ã«å®Ÿè¡Œã•ã‚ŒãŸå…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¸€è¦§ã‚’å–å¾—ã™ã‚‹",
    inputSchema: {},
  },
  async () => {
    const sessions = thoughtManager.getAllSessions();

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify({
            sessions: sessions.map(s => ({
              sessionId: s.sessionId,
              responsesCount: s.responses.length,
              totalDuration: s.totalDuration,
              timestamp: s.timestamp,
              hasSummary: !!s.summary,
              hasConsensus: !!s.consensus
            })),
            totalSessions: sessions.length,
            timestamp: Date.now()
          }, null, 2)
        }
      ]
    };
  }
);

// ãƒ„ãƒ¼ãƒ«7: æ ¼å®‰LLMã¸ã®å§”è­²ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
server.registerTool(
  "delegate-to-cheap-llm",
  {
    title: "æ ¼å®‰LLMå§”è­²",
    description: "ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã‚’æ ¼å®‰LLMï¼ˆDeepSeekã€Ollamaç­‰ï¼‰ã«å§”è­²ã™ã‚‹",
    inputSchema: {
      task: z.string().describe("å§”è­²ã—ãŸã„ã‚¿ã‚¹ã‚¯ã®èª¬æ˜"),
      provider: z.enum(['deepseek', 'ollama']).optional().describe("ä½¿ç”¨ã™ã‚‹æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯åˆ©ç”¨å¯èƒ½ãªæœ€ã‚‚å®‰ã„ã‚‚ã®ã‚’è‡ªå‹•é¸æŠï¼‰"),
      model: z.string().optional().describe("ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«æŒ‡å®š"),
      temperature: z.number().min(0).max(2).optional().default(0.3).describe("å‰µé€ æ€§ãƒ¬ãƒ™ãƒ«ï¼ˆç¯€ç´„é‡è¦–ã§ä½ã‚ã«è¨­å®šï¼‰"),
      maxTokens: z.number().min(1).max(2000).optional().default(1000).describe("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆç¯€ç´„é‡è¦–ã§ä½ã‚ã«è¨­å®šï¼‰")
    },
  },
  async ({ task, provider, model, temperature, maxTokens }) => {
    const availableProviders = thoughtManager.getAvailableProviders();
    const cheapProviders = ['deepseek', 'ollama'].filter(p => availableProviders.includes(p as AIProvider));
    
    if (cheapProviders.length === 0) {
      throw new Error("æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆDeepSeekã€Ollamaï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚");
    }

    const targetProvider = provider && cheapProviders.includes(provider) 
      ? provider 
      : cheapProviders[0]; // æœ€åˆã«åˆ©ç”¨å¯èƒ½ãªæ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨

    const delegationTask: AIThoughtTask = {
      id: `delegate-${Date.now()}`,
      prompt: `ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ç°¡æ½”ã§å®Ÿç”¨çš„ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼š

${task}`,
      provider: targetProvider as AIProvider,
      model,
      temperature,
      maxTokens
    };

    try {
      const response = await thoughtManager.executeTask(delegationTask);

      return {
        content: [
          {
            type: "text",
            text: JSON.stringify({
              task,
              delegatedTo: targetProvider,
              model: response.model,
              result: response.response,
              tokenUsage: response.usage,
              duration: response.duration,
              costSaving: "é«˜é¡ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ä½¿ç”¨ã‚’å›é¿ã—ã¦ã‚³ã‚¹ãƒˆã‚’ç¯€ç´„ã—ã¾ã—ãŸ",
              timestamp: response.timestamp
            }, null, 2)
          }
        ]
      };
    } catch (error) {
      throw new Error(`æ ¼å®‰LLMå§”è­²ã«å¤±æ•—ã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
);

// ãƒ„ãƒ¼ãƒ«8: ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆâ†’èª¿æ•´ï¼ˆæ®µéšçš„å‡¦ç†ï¼‰
server.registerTool(
  "draft-and-refine",
  {
    title: "ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆâ†’èª¿æ•´",
    description: "æ ¼å®‰LLMã§ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆå¾Œã€é«˜å“è³ªLLMã§èª¿æ•´ã™ã‚‹æ®µéšçš„å‡¦ç†ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¯€ç´„",
    inputSchema: {
      task: z.string().describe("ä½œæˆã—ãŸã„å†…å®¹ã®èª¬æ˜"),
      cheapProvider: z.enum(['deepseek', 'ollama']).optional().describe("ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆç”¨ã®æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"),
      refineProvider: z.enum(['openai', 'anthropic', 'gemini']).optional().describe("èª¿æ•´ç”¨ã®é«˜å“è³ªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"),
      draftMaxTokens: z.number().min(1).max(2000).optional().default(1000).describe("ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆæ™‚ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"),
      refineMaxTokens: z.number().min(1).max(1500).optional().default(800).describe("èª¿æ•´æ™‚ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    },
  },
  async ({ task, cheapProvider, refineProvider, draftMaxTokens, refineMaxTokens }) => {
    const availableProviders = thoughtManager.getAvailableProviders();
    const cheapProviders = ['deepseek', 'ollama'].filter(p => availableProviders.includes(p as AIProvider));
    const premiumProviders = ['openai', 'anthropic', 'gemini'].filter(p => availableProviders.includes(p as AIProvider));
    
    if (cheapProviders.length === 0) {
      throw new Error("æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚");
    }
    if (premiumProviders.length === 0) {
      throw new Error("é«˜å“è³ªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚");
    }

    const selectedCheap = cheapProvider && cheapProviders.includes(cheapProvider) 
      ? cheapProvider : cheapProviders[0];
    const selectedPremium = refineProvider && premiumProviders.includes(refineProvider) 
      ? refineProvider : premiumProviders[0];

    // Step 1: æ ¼å®‰LLMã§ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ
    const draftTask: AIThoughtTask = {
      id: `draft-${Date.now()}`,
      prompt: `ä»¥ä¸‹ã®è¦æ±‚ã«å¯¾ã—ã¦ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚å®Œç’§ã§ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšã¯åŸºæœ¬çš„ãªæ§‹é€ ã¨å†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

${task}`,
      provider: selectedCheap as AIProvider,
      temperature: 0.7,
      maxTokens: draftMaxTokens
    };

    const draftResponse = await thoughtManager.executeTask(draftTask);

    // Step 2: é«˜å“è³ªLLMã§èª¿æ•´
    const refineTask: AIThoughtTask = {
      id: `refine-${Date.now()}`,
      prompt: `ä»¥ä¸‹ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’æ”¹å–„ãƒ»èª¿æ•´ã—ã¦ãã ã•ã„ã€‚å†…å®¹ã®ç²¾åº¦å‘ä¸Šã€è¡¨ç¾ã®æ”¹å–„ã€æ§‹é€ ã®æœ€é©åŒ–ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

ã€å…ƒã®è¦æ±‚ã€‘
${task}

ã€ãƒ‰ãƒ©ãƒ•ãƒˆã€‘
${draftResponse.response}

æ”¹å–„ã•ã‚ŒãŸæœ€çµ‚ç‰ˆã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š`,
      provider: selectedPremium as AIProvider,
      temperature: 0.3,
      maxTokens: refineMaxTokens
    };

    const refinedResponse = await thoughtManager.executeTask(refineTask);

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify({
            task,
            process: "æ®µéšçš„å‡¦ç†",
            draftPhase: {
              provider: selectedCheap,
              model: draftResponse.model,
              tokens: draftResponse.usage,
              duration: draftResponse.duration
            },
            refinePhase: {
              provider: selectedPremium,
              model: refinedResponse.model,
              tokens: refinedResponse.usage,
              duration: refinedResponse.duration
            },
            draft: draftResponse.response,
            finalResult: refinedResponse.response,
            totalDuration: draftResponse.duration + refinedResponse.duration,
            costOptimization: "æ ¼å®‰LLMã§ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆâ†’é«˜å“è³ªLLMã§èª¿æ•´ã®æ®µéšçš„å‡¦ç†ã«ã‚ˆã‚Šã€ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’æœ€é©åŒ–ã—ã¾ã—ãŸ"
          }, null, 2)
        }
      ]
    };
  }
);

// ãƒ„ãƒ¼ãƒ«9: é•·æ–‡è¦ç´„ï¼ˆå‰å‡¦ç†ï¼‰
server.registerTool(
  "summarize-for-efficiency",
  {
    title: "åŠ¹ç‡çš„è¦ç´„",
    description: "é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¼å®‰LLMã§è¦ç´„ã—ã¦ã€ãƒ¡ã‚¤ãƒ³LLMã®ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’å‰Šæ¸›ã™ã‚‹å‰å‡¦ç†",
    inputSchema: {
      text: z.string().describe("è¦ç´„ã—ãŸã„é•·ã„ãƒ†ã‚­ã‚¹ãƒˆ"),
      summaryLength: z.enum(['short', 'medium', 'detailed']).optional().default('medium').describe("è¦ç´„ã®é•·ã•"),
      provider: z.enum(['deepseek', 'ollama']).optional().describe("è¦ç´„ç”¨ã®æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"),
      focus: z.string().optional().describe("è¦ç´„æ™‚ã«é‡ç‚¹ã‚’ç½®ãè¦³ç‚¹ã‚„ãƒ†ãƒ¼ãƒ")
    },
  },
  async ({ text, summaryLength, provider, focus }) => {
    const availableProviders = thoughtManager.getAvailableProviders();
    const cheapProviders = ['deepseek', 'ollama'].filter(p => availableProviders.includes(p as AIProvider));
    
    if (cheapProviders.length === 0) {
      throw new Error("æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚");
    }

    const targetProvider = provider && cheapProviders.includes(provider) 
      ? provider : cheapProviders[0];

    const lengthInstructions = {
      short: "3-5æ–‡ã§ç°¡æ½”ã«",
      medium: "1-2æ®µè½ã§é©åº¦ã«è©³ã—ã",
      detailed: "3-4æ®µè½ã§è©³ç´°ã«"
    };

    const summaryPrompt = `ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’${lengthInstructions[summaryLength]}è¦ç´„ã—ã¦ãã ã•ã„ã€‚${
      focus ? `ç‰¹ã«ã€Œ${focus}ã€ã®è¦³ç‚¹ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚` : ""
    }

ã€è¦ç´„å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã€‘
${text}

è¦ç´„ï¼š`;

    const summaryTask: AIThoughtTask = {
      id: `summary-${Date.now()}`,
      prompt: summaryPrompt,
      provider: targetProvider as AIProvider,
      temperature: 0.1,
      maxTokens: summaryLength === 'short' ? 200 : summaryLength === 'medium' ? 400 : 600
    };

    const response = await thoughtManager.executeTask(summaryTask);

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify({
            originalLength: text.length,
          summaryLength: response.response.length,
          compressionRatio: `${Math.round((1 - response.response.length / text.length) * 100)}%å‰Šæ¸›`,
          summary: response.response,
          provider: targetProvider,
          tokenUsage: response.usage,
          duration: response.duration,
          efficiency: "é•·æ–‡ã‚’æ ¼å®‰LLMã§è¦ç´„ã™ã‚‹ã“ã¨ã§ã€å¾Œç¶šå‡¦ç†ã®ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã—ãŸ"
        }, null, 2)
        }
      ]
    };
  }
);

// ãƒ„ãƒ¼ãƒ«10: ãƒãƒƒãƒå‡¦ç†ï¼ˆè¤‡æ•°ã‚¿ã‚¹ã‚¯ã®ä¸€æ‹¬å‡¦ç†ï¼‰
server.registerTool(
  "batch-process-cheap",
  {
    title: "ãƒãƒƒãƒå‡¦ç†",
    description: "è¤‡æ•°ã®å˜ç´”ãªã‚¿ã‚¹ã‚¯ã‚’æ ¼å®‰LLMã§ä¸€æ‹¬å‡¦ç†ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¯€ç´„",
    inputSchema: {
      tasks: z.array(z.string()).describe("å‡¦ç†ã—ãŸã„ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ"),
      provider: z.enum(['deepseek', 'ollama']).optional().describe("ä½¿ç”¨ã™ã‚‹æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"),
      maxTokensPerTask: z.number().min(50).max(500).optional().default(200).describe("ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    },
  },
  async ({ tasks, provider, maxTokensPerTask }) => {
    const availableProviders = thoughtManager.getAvailableProviders();
    const cheapProviders = ['deepseek', 'ollama'].filter(p => availableProviders.includes(p as AIProvider));
    
    if (cheapProviders.length === 0) {
      throw new Error("æ ¼å®‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚");
    }

    const targetProvider = provider && cheapProviders.includes(provider) 
      ? provider : cheapProviders[0];

    const batchPrompt = `ä»¥ä¸‹ã®${tasks.length}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’é †ç•ªã«å‡¦ç†ã—ã¦ãã ã•ã„ã€‚å„ã‚¿ã‚¹ã‚¯ã®å›ç­”ã¯ç°¡æ½”ã§å®Ÿç”¨çš„ã«ã—ã¦ãã ã•ã„ï¼š

${tasks.map((task, index) => `${index + 1}. ${task}`).join('\n')}

å„ã‚¿ã‚¹ã‚¯ã®å›ç­”ã‚’ä»¥ä¸‹ã®å½¢å¼ã§æä¾›ã—ã¦ãã ã•ã„ï¼š
ã€ã‚¿ã‚¹ã‚¯1ã®å›ç­”ã€‘
ï¼ˆå›ç­”å†…å®¹ï¼‰

ã€ã‚¿ã‚¹ã‚¯2ã®å›ç­”ã€‘
ï¼ˆå›ç­”å†…å®¹ï¼‰

...`;

    const batchTask: AIThoughtTask = {
      id: `batch-${Date.now()}`,
      prompt: batchPrompt,
      provider: targetProvider as AIProvider,
      temperature: 0.3,
      maxTokens: Math.min(tasks.length * maxTokensPerTask, 2000)
    };

    const response = await thoughtManager.executeTask(batchTask);

    return {
      content: [
        {
          type: "text",
          text: JSON.stringify({
            tasksCount: tasks.length,
            batchResult: response.response,
            provider: targetProvider,
            model: response.model,
            tokenUsage: response.usage,
            duration: response.duration,
            efficiency: `${tasks.length}å€‹ã®ã‚¿ã‚¹ã‚¯ã‚’1å›ã®APIå‘¼ã³å‡ºã—ã§å‡¦ç†ã—ã€å¤§å¹…ãªãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã‚’å®Ÿç¾ã—ã¾ã—ãŸ`
          }, null, 2)
        }
      ]
    };
  }
);

// ã‚µãƒ¼ãƒãƒ¼èµ·å‹•å‡¦ç†
async function main() {
  const availableProviders = thoughtManager.getAvailableProviders();
  
  if (availableProviders.length === 0) {
    console.error("è­¦å‘Š: åˆ©ç”¨å¯èƒ½ãªAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚");
    console.error("ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:");
    console.error("- OPENAI_API_KEY");
    console.error("- ANTHROPIC_API_KEY");
    console.error("- GEMINI_API_KEY");
    console.error("- DEEPSEEK_API_KEY");
    console.error("- OLLAMA_BASE_URL");
  } else {
    console.error(`ğŸ§  åˆ©ç”¨å¯èƒ½ãªAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ${availableProviders.join(', ')}`);
  }

  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error("ğŸ¤– ä¸¦è¡ŒAIæ€è€ƒã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¾ã—ãŸ");
}

main().catch((err) => {
  console.error("Fatal error:", err);
  process.exit(1);
});
